name: "[GPU] mostlyai-engine Tests"

on:
    workflow_call:


env:
    PYTHON_KEYRING_BACKEND: keyring.backends.null.Keyring
    FORCE_COLOR: "1"

jobs:
    run-tests-gpu:
        runs-on: gha-gpu-public-nvidia
        permissions:
            contents: read
            packages: write
        steps:
          - name: Setup | Checkout
            uses: actions/checkout@v4
            with:
                fetch-depth: 0
                submodules: 'recursive'

          - name: Setup | uv
            uses: astral-sh/setup-uv@v5
            with:
              enable-cache: false
              python-version: '3.10'

          # - name: Debug | Verify CUDA installation
          #   run: |
          #     nvcc --version && \
          #     echo "Installed CUDA version is: ${{ steps.cuda-toolkit.outputs.cuda }}" && \
          #     echo "CUDA installation path is: ${{ steps.cuda-toolkit.outputs.CUDA_PATH }}" || \
          #     echo "CUDA not found"

          - name: Setup | Dependencies
            run: uv sync --frozen --extra gpu --no-group docs

          - name: Setup | Install CUDA
            id: cuda-toolkit
            uses: Jimver/cuda-toolkit@v0.2.21
            with:
              cuda: '12.4.0'

          - name: Setup | Verify CUDA installation
            run: |
              nvcc --version && \
              ls -l /usr/local/ && \
              echo "Installed CUDA version is: ${{ steps.cuda-toolkit.outputs.cuda }}" && \
              echo "CUDA installation path is: ${{ steps.cuda-toolkit.outputs.CUDA_PATH }}" || \
              echo "CUDA not found"

          - name: Debug | Check PyTorch and CUDA version
            run: |
              python -c "import torch; print('Torch:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda)"

          - name: Debug | Check available GPU drivers
            run: ls /dev/nvidia* && lsmod | grep nvidia || echo "No NVIDIA kernel module found"

          - name: Setup | Check for available GPU-s
            run: nvidia-smi

          - name: Run tests -> end_to_end -> sequential
            run: uv run pytest --log-cli-level=INFO tests/end_to_end/test_tabular_sequential.py

          - name: Run tests -> end_to_end -> sequential context
            run: uv run pytest --log-cli-level=INFO tests/end_to_end/test_tabular_sequential_context.py

          - name: Run tests -> end_to_end all except sequential
            run: uv run pytest --log-cli-level=INFO --ignore=tests/end_to_end/test_tabular_sequential.py --ignore=tests/end_to_end/test_tabular_sequential_context.py tests/end_to_end/
