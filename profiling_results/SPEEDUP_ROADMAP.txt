================================================================================
                    PERFORMANCE OPTIMIZATION ROADMAP
================================================================================

CURRENT PERFORMANCE (100K rows, 14 mixed-type columns, n_jobs=1)
================================================================================
ANALYZE: 11.47s  (8,718 rows/sec)   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 79%]
ENCODE:   3.14s  (31,885 rows/sec)  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 21%]
TOTAL:   14.61s
================================================================================


TOP 5 BOTTLENECKS (by time spent)
================================================================================
1. compute_log_histogram        4.06s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 28%]  ğŸ”´ CRITICAL
2. split_sub_columns_digit      3.69s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 25%]   ğŸ”´ CRITICAL
3. String splitting (regex)     3.06s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 21%]     ğŸŸ¡ HIGH
4. is_a_list (3M calls!)        1.09s  [â–ˆâ–ˆâ–ˆâ–ˆ 7%]              ğŸ”´ CRITICAL
5. is_sequential checks         1.29s  [â–ˆâ–ˆâ–ˆâ–ˆ 9%]              ğŸŸ¡ HIGH
   Other                        1.42s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10%]
================================================================================


OPTIMIZATION PHASES
================================================================================

PHASE 1: QUICK WINS (1-2 days)                     Expected: 14.61s â†’ ~11s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… 1. Column-level sequential detection           Saves: 1.09s
   - Replace 3M is_a_list calls with 14 checks
   - File: mostlyai/engine/_common.py:201

âœ… 2. Sample-based is_sequential                   Saves: 1.00s
   - Check first 100 values instead of all
   - File: mostlyai/engine/_common.py:205

âœ… 3. Cache histogram edges                        Saves: 0.50s
   - Reuse bins across similar columns
   - File: mostlyai/engine/_common.py:636

âœ… 4. Reduce type conversions                      Saves: 0.30s
   - Minimize astype() calls
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUBTOTAL:                                          Saves: 2.89s (~20%)


PHASE 2: MAJOR OPTIMIZATIONS (3-5 days)            Expected: 11s â†’ ~3.5s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… 1. Vectorize compute_log_histogram              Saves: 3.50s
   - Use np.histogram() instead of Python loops
   - Consider numba JIT for additional speedup
   - File: mostlyai/engine/_common.py:636

âœ… 2. Math-based digit splitting                   Saves: 3.00s
   - Use (val // 10**pos) % 10 instead of str ops
   - Add numba JIT for parallel processing
   - File: mostlyai/engine/_encoding_types/tabular/numeric.py:108

âœ… 3. Vectorize string splitting                   Saves: 2.00s
   - Use numpy array views instead of regex
   - File: mostlyai/engine/_encoding_types/tabular/character.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUBTOTAL:                                          Saves: 8.50s (~58%)


PHASE 3: ADVANCED (5-10 days)                      Expected: 3.5s â†’ ~1.7s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Numba/Cython for hot loops                      Saves: 1.00s
âœ… Zero-copy operations                            Saves: 0.50s
âœ… Memory layout optimization                      Saves: 0.30s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUBTOTAL:                                          Saves: 1.80s (~12%)


PHASE 4: PARALLELIZATION (with n_jobs=4)          Expected: 1.7s â†’ ~0.6s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Column-level parallelization                    3-4x speedup
âœ… Shared memory for DataFrames                    Additional 1.2x
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUBTOTAL:                                          Additional 3-4x


================================================================================
                        PROJECTED PERFORMANCE
================================================================================

CURRENT:        14.61s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
After Phase 1:  11.00s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]          75%
After Phase 2:   3.50s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]                               24%
After Phase 3:   1.70s  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]                                    12%
After Phase 4:   0.60s  [â–ˆâ–ˆ]                                        4%

TOTAL SPEEDUP: 24x faster  (14.61s â†’ 0.60s)

Throughput improvement:
  - Current:  6,800 rows/sec
  - Final:  166,000 rows/sec  (24x)


================================================================================
                    OPTIMIZATION IMPLEMENTATION
================================================================================

See detailed examples in: optimization_examples.py

Example - Optimized compute_log_histogram:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ def compute_log_histogram_optimized(values, n_bins=100):          â”‚
â”‚     log_vals = np.log10(np.abs(values) + 1e-10)                   â”‚
â”‚     counts, edges = np.histogram(log_vals, bins=n_bins)           â”‚
â”‚     return {"counts": counts.tolist(), "edges": edges.tolist()}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example - Optimized digit splitting:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ @numba.jit(nopython=True, parallel=True)                           â”‚
â”‚ def extract_digits(values, max_digits):                            â”‚
â”‚     result = np.zeros((len(values), max_digits), dtype=np.int8)   â”‚
â”‚     for i in numba.prange(len(values)):                            â”‚
â”‚         val = abs(int(values[i]))                                  â”‚
â”‚         for j in range(max_digits):                                â”‚
â”‚             result[i, j] = (val // 10**j) % 10                     â”‚
â”‚     return result                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example - Eliminate is_a_list checks:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # Before: Check every value (3M times)                             â”‚
â”‚ for val in series:                                                 â”‚
â”‚     if is_a_list(val): ...  # Slow!                                â”‚
â”‚                                                                     â”‚
â”‚ # After: Check once per column (14 times)                          â”‚
â”‚ is_seq = is_column_sequential(series)  # Check first few values   â”‚
â”‚ if is_seq:                                                          â”‚
â”‚     # Process as sequential                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
                        FILES TO MODIFY
================================================================================

Priority 1 (Critical Impact):
  1. mostlyai/engine/_common.py
     - compute_log_histogram() [line 636] â†’ Save 3.5s
     - is_a_list() [line 201]             â†’ Save 1.1s
     - is_sequential() [line 205]         â†’ Save 1.0s

  2. mostlyai/engine/_encoding_types/tabular/numeric.py
     - split_sub_columns_digit() [line 108] â†’ Save 3.0s

Priority 2 (High Impact):
  3. mostlyai/engine/_encoding_types/tabular/character.py
     - String splitting operations â†’ Save 2.0s

  4. mostlyai/engine/_tabular/encoding.py
     - encode_df() [line 181] â†’ Refactor sequential detection

  5. mostlyai/engine/analysis.py
     - _analyze_col() [line 514] â†’ Optimize column analysis


================================================================================
                        TESTING & VALIDATION
================================================================================

1. Unit Tests
   âœ“ Test each optimized function independently
   âœ“ Ensure output matches original implementation
   âœ“ Test edge cases (empty, nulls, single row)

2. Integration Tests
   âœ“ Run full analyze + encode pipeline
   âœ“ Compare stats.json files (should be identical)
   âœ“ Verify encoded data correctness

3. Performance Tests
   âœ“ Profile before/after each optimization
   âœ“ Measure speedup on various dataset sizes
   âœ“ Test with different column type mixes

4. Regression Tests
   âœ“ Run existing test suite
   âœ“ Ensure no behavioral changes


================================================================================
                        NEXT STEPS
================================================================================

1. âœ… Review PROFILING_ANALYSIS.md for detailed analysis
2. âœ… Study optimization_examples.py for implementation examples
3. â¬œ Implement Phase 1 (Quick Wins) - 1-2 days
4. â¬œ Measure actual speedup achieved
5. â¬œ Implement Phase 2 (Major Optimizations) - 3-5 days
6. â¬œ Re-profile and measure improvements
7. â¬œ Consider Phase 3/4 based on requirements


================================================================================
                        PROFILING ARTIFACTS
================================================================================

Generated Files:
  âœ“ profiling_results/analyze_profile.txt  - Detailed ANALYZE profile
  âœ“ profiling_results/encode_profile.txt   - Detailed ENCODE profile
  âœ“ PROFILING_ANALYSIS.md                  - Complete analysis
  âœ“ PROFILING_SUMMARY.md                   - Executive summary
  âœ“ optimization_examples.py               - Implementation examples
  âœ“ profile_analyze_encode_v2.py           - Profiling script

To re-run profiling:
  $ uv run python profile_analyze_encode_v2.py

To test optimizations:
  $ uv run python optimization_examples.py


================================================================================
                            END OF REPORT
================================================================================
