{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tabular Model: sequential data, with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mostly-ai/mostlyai-engine/blob/main/examples/sequential.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mostlyai import engine\n",
    "\n",
    "# init workspace and logging\n",
    "ws = Path(\"ws-tabular-sequential\")\n",
    "engine.init_logging()\n",
    "\n",
    "# load original data\n",
    "url = \"https://github.com/mostly-ai/public-demo-data/raw/refs/heads/dev/baseball\"\n",
    "trn_ctx_df = pd.read_csv(f\"{url}/players.csv.gz\")  # context data\n",
    "trn_tgt_df = pd.read_csv(f\"{url}/batting.csv.gz\")  # target data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>birthDate</th>\n",
       "      <th>deathDate</th>\n",
       "      <th>nameFirst</th>\n",
       "      <th>nameLast</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>bats</th>\n",
       "      <th>throws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardsda01</td>\n",
       "      <td>USA</td>\n",
       "      <td>1981-12-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>David</td>\n",
       "      <td>Aardsma</td>\n",
       "      <td>215.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaronha01</td>\n",
       "      <td>USA</td>\n",
       "      <td>1934-02-05</td>\n",
       "      <td>2021-01-22</td>\n",
       "      <td>Hank</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>180.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaronto01</td>\n",
       "      <td>USA</td>\n",
       "      <td>1939-08-05</td>\n",
       "      <td>1984-08-16</td>\n",
       "      <td>Tommie</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>190.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aasedo01</td>\n",
       "      <td>USA</td>\n",
       "      <td>1954-09-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Don</td>\n",
       "      <td>Aase</td>\n",
       "      <td>190.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abadan01</td>\n",
       "      <td>USA</td>\n",
       "      <td>1972-08-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Andy</td>\n",
       "      <td>Abad</td>\n",
       "      <td>184.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id country   birthDate   deathDate nameFirst nameLast  weight  \\\n",
       "0  aardsda01     USA  1981-12-27         NaN     David  Aardsma   215.0   \n",
       "1  aaronha01     USA  1934-02-05  2021-01-22      Hank    Aaron   180.0   \n",
       "2  aaronto01     USA  1939-08-05  1984-08-16    Tommie    Aaron   190.0   \n",
       "3   aasedo01     USA  1954-09-08         NaN       Don     Aase   190.0   \n",
       "4   abadan01     USA  1972-08-25         NaN      Andy     Abad   184.0   \n",
       "\n",
       "   height bats throws  \n",
       "0    75.0    R      R  \n",
       "1    72.0    R      R  \n",
       "2    75.0    R      R  \n",
       "3    75.0    R      R  \n",
       "4    73.0    L      L  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ctx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-09 11:28:30,443] INFO   : SPLIT started\n",
      "[2025-07-09 11:28:30,447] INFO   : clean `ws-tabular-sequential/OriginalData/tgt-data`\n",
      "[2025-07-09 11:28:30,448] INFO   : clean `ws-tabular-sequential/OriginalData/tgt-meta`\n",
      "[2025-07-09 11:28:30,448] INFO   : clean `ws-tabular-sequential/OriginalData/ctx-data`\n",
      "[2025-07-09 11:28:30,449] INFO   : clean `ws-tabular-sequential/OriginalData/ctx-meta`\n",
      "[2025-07-09 11:28:30,449] INFO   : model_type='TABULAR'\n",
      "[2025-07-09 11:28:30,449] INFO   : tgt_encoding_types={'date': 'TABULAR_CATEGORICAL', 'cds': 'TABULAR_NUMERIC_AUTO', 'amt': 'TABULAR_NUMERIC_AUTO'}\n",
      "[2025-07-09 11:28:30,469] INFO   : SPLIT finished in 0.02s\n",
      "[2025-07-09 11:28:30,469] INFO   : ANALYZE started\n",
      "[2025-07-09 11:28:30,470] INFO   : clean `ws-tabular-sequential/ModelStore/tgt-stats`\n",
      "[2025-07-09 11:28:30,471] INFO   : clean `ws-tabular-sequential/ModelStore/ctx-stats`\n",
      "[2025-07-09 11:28:30,472] INFO   : analyzing 2 partitions in parallel\n",
      "[2025-07-09 11:28:31,081] INFO   : analyzed target partition 000000-trn (55837, 4)\n",
      "[2025-07-09 11:28:31,084] INFO   : analyzed context partition 000000-trn (18856, 1)\n",
      "[2025-07-09 11:28:31,230] INFO   : analyzed target partition 000000-val (13822, 4)\n",
      "[2025-07-09 11:28:31,232] INFO   : analyzed context partition 000000-val (4714, 1)\n",
      "[2025-07-09 11:28:31,233] INFO   : combine partition statistics\n",
      "[2025-07-09 11:28:31,235] INFO   : value_protection = True\n",
      "[2025-07-09 11:28:31,235] INFO   : analyzed column `date`: TABULAR_CATEGORICAL {'cat': 547}\n",
      "[2025-07-09 11:28:31,235] INFO   : analyzed column `cds`: TABULAR_NUMERIC_DISCRETE {'cat': 27}\n",
      "[2025-07-09 11:28:31,237] INFO   : analyzed column `amt`: TABULAR_NUMERIC_BINNED {'bin': 103}\n",
      "[2025-07-09 11:28:31,237] INFO   : analyzed 23,570 records: 18,856 training / 4,714 validation\n",
      "[2025-07-09 11:28:31,237] INFO   : is_sequential: True\n",
      "[2025-07-09 11:28:31,238] INFO   : write statistics to `ws-tabular-sequential/ModelStore/tgt-stats/stats.json`\n",
      "[2025-07-09 11:28:31,239] INFO   : value_protection = True\n",
      "[2025-07-09 11:28:31,240] INFO   : write statistics to `ws-tabular-sequential/ModelStore/ctx-stats/stats.json`\n",
      "[2025-07-09 11:28:31,240] INFO   : ANALYZE finished in 0.77s\n",
      "[2025-07-09 11:28:31,241] INFO   : ENCODE_TABULAR started\n",
      "[2025-07-09 11:28:31,242] INFO   : clean `ws-tabular-sequential/OriginalData/encoded-data`\n",
      "[2025-07-09 11:28:31,242] INFO   : clean `ws-tabular-sequential/OriginalData/encoded-data`\n",
      "[2025-07-09 11:28:31,243] INFO   : clean `ws-tabular-sequential/OriginalData/encoded-data`\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tgt:/__sidx_cat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/mostlyai-engine/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'tgt:/__sidx_cat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m engine.split(                         \u001b[38;5;66;03m# split data as PQT files for `trn` + `val` to `{ws}/OriginalData/(tgt|ctx)-data`\u001b[39;00m\n\u001b[32m      3\u001b[39m   workspace_dir=ws,\n\u001b[32m      4\u001b[39m   tgt_data=trn_tgt_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m   model_type=\u001b[33m\"\u001b[39m\u001b[33mTABULAR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m engine.analyze(workspace_dir=ws)      \u001b[38;5;66;03m# generate column-level statistics to `{ws}/ModelStore/(tgt|ctx)-data/stats.json`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# encode training data to `{ws}/OriginalData/encoded-data`\u001b[39;00m\n\u001b[32m     12\u001b[39m engine.train(                         \u001b[38;5;66;03m# train model and store to `{ws}/ModelStore/model-data`\u001b[39;00m\n\u001b[32m     13\u001b[39m     workspace_dir=ws,\n\u001b[32m     14\u001b[39m     max_training_time=\u001b[32m2\u001b[39m,              \u001b[38;5;66;03m# limit TRAIN to 2 minute for demo purposes\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m engine.generate(workspace_dir=ws)     \u001b[38;5;66;03m# use model to generate synthetic samples to `{ws}/SyntheticData`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/mostlyai-engine/mostlyai/engine/encoding.py:42\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(workspace_dir, update_progress)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type == ModelType.tabular:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmostlyai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tabular\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m encode \u001b[38;5;28;01mas\u001b[39;00m encode_tabular\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_tabular\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkspace_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmostlyai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_language\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m encode \u001b[38;5;28;01mas\u001b[39;00m encode_language\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/mostlyai-engine/mostlyai/engine/_tabular/encoding.py:80\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(workspace_dir, update_progress)\u001b[39m\n\u001b[32m     77\u001b[39m     ctx_stats = workspace.ctx_stats.read()\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tgt_pqt_partitions)):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         \u001b[43m_encode_partition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtgt_partition_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_pqt_partitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtgt_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoded_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctx_partition_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_pqt_partitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhas_context\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctx_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhas_context\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m         progress.update(completed=i, total=\u001b[38;5;28mlen\u001b[39m(tgt_pqt_partitions) + \u001b[32m1\u001b[39m)\n\u001b[32m     89\u001b[39m _LOG.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mENCODE_TABULAR finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/mostlyai-engine/mostlyai/engine/_tabular/encoding.py:154\u001b[39m, in \u001b[36m_encode_partition\u001b[39m\u001b[34m(tgt_partition_file, tgt_stats, output_path, ctx_partition_file, ctx_stats, n_jobs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# pad each list with one extra item\u001b[39;00m\n\u001b[32m    153\u001b[39m     df = pad_horizontally(df, padding_value=\u001b[32m0\u001b[39m, right=\u001b[38;5;28;01mTrue\u001b[39;00m, pad_only_0seqlens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     df[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSIDX_SUB_COLUMN_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mcat\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSIDX_SUB_COLUMN_PREFIX\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mcat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[:-\u001b[32m1\u001b[39m] + [x[-\u001b[32m2\u001b[39m] + \u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) >= \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[32m1\u001b[39m]\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m has_context:\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# add 0-rows for IDs, that are present in context, but not in target; i.e., for zero-sequence records\u001b[39;00m\n\u001b[32m    159\u001b[39m     zero_seq_ids = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(df_ctx[ctx_primary_key]) - \u001b[38;5;28mset\u001b[39m(df[tgt_context_key]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/mostlyai-engine/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/mostlyai-engine/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'tgt:/__sidx_cat'"
     ]
    }
   ],
   "source": [
    "# execute the engine steps\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mostlyai import engine\n",
    "\n",
    "# init workspace and logging\n",
    "ws = Path(\"ws-tabular-sequential\")\n",
    "engine.init_logging()\n",
    "\n",
    "# load original data\n",
    "url = \"https://github.com/mostly-ai/public-demo-data/raw/refs/heads/dev/cdnow\"\n",
    "trn_ctx_df = pd.read_csv(f\"{url}/users.csv.gz\")  # context data\n",
    "trn_tgt_df = pd.read_csv(f\"{url}/purchases.csv.gz\")  # target data\n",
    "\n",
    "engine.split(                         # split data as PQT files for `trn` + `val` to `{ws}/OriginalData/(tgt|ctx)-data`\n",
    "  workspace_dir=ws,\n",
    "  tgt_data=trn_tgt_df,\n",
    "  ctx_data=trn_ctx_df,\n",
    "  tgt_context_key=\"users_id\",\n",
    "  ctx_primary_key=\"id\",\n",
    "  model_type=\"TABULAR\",\n",
    ")\n",
    "engine.analyze(workspace_dir=ws)      # generate column-level statistics to `{ws}/ModelStore/(tgt|ctx)-data/stats.json`\n",
    "engine.encode(workspace_dir=ws)       # encode training data to `{ws}/OriginalData/encoded-data`\n",
    "engine.train(                         # train model and store to `{ws}/ModelStore/model-data`\n",
    "    workspace_dir=ws,\n",
    "    max_training_time=2,              # limit TRAIN to 2 minute for demo purposes\n",
    ")\n",
    "engine.generate(workspace_dir=ws)     # use model to generate synthetic samples to `{ws}/SyntheticData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic data\n",
    "syn_tgt_df = pd.read_parquet(ws / \"SyntheticData\")\n",
    "syn_tgt_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUALITY ASSURANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn_seq_lens = trn_tgt_df.groupby(\"players_id\").size()\n",
    "syn_seq_lens = syn_tgt_df.groupby(\"players_id\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tgt: \", np.quantile(trn_seq_lens, np.arange(0, 1.1, 0.1), method=\"inverted_cdf\"))\n",
    "print(\"syn: \", np.quantile(syn_seq_lens, np.arange(0, 1.1, 0.1), method=\"inverted_cdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_avg_teams_per_player = syn_tgt_df.groupby(\"players_id\")[\"team\"].nunique().mean().round(1)\n",
    "trn_avg_teams_per_player = trn_tgt_df.groupby(\"players_id\")[\"team\"].nunique().mean().round(1)\n",
    "syn_avg_teams_per_player, trn_avg_teams_per_player"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
